{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary for the Similarity Modeling 2 assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timesheets for the lectures and summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the time spent on the project, please refer to the specific notebooks. The following tables are dedicated to the time spent on watching the lectures and writing the summaries.\n",
    "\n",
    "**Daniel Blasko:**\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>Date</th>\n",
    "    <th>Task</th>\n",
    "    <th>Hours</th>\n",
    "\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>13.10.23</td>\n",
    "    <td>Watch \"Setting\" video</td>\n",
    "    <td>1h20</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>14.10.23</td>\n",
    "    <td>Write \"Setting\" summary</td>\n",
    "    <td>1h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>13.10.23</td>\n",
    "    <td>Watch \"Similarity Measurement\" video</td>\n",
    "    <td>1h10</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>14.10.23</td>\n",
    "    <td>Write \"Similarity Measurement\" summary</td>\n",
    "    <td>50min</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>14.10.23</td>\n",
    "    <td>Watch \"Feature engineering\" video</td>\n",
    "    <td>1h40</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>15.10.23</td>\n",
    "    <td>Write \"Feature engineering\" summary</td>\n",
    "    <td>1h10</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>18.10.23</td>\n",
    "    <td>Watch \"Classification\" video</td>\n",
    "    <td>1h50</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>19.10.23</td>\n",
    "    <td>Write \"Classification\" summary</td>\n",
    "    <td>1h20</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>18.10.23</td>\n",
    "    <td>Watch \"Evaluation\" video</td>\n",
    "    <td>45min</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>19.10.23</td>\n",
    "    <td>Write \"Evaluation\" summary</td>\n",
    "    <td>50min</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>19.10.23</td>\n",
    "    <td>Watch \"Perception & Psychophysics\" video</td>\n",
    "    <td>1h50</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>20.10.23</td>\n",
    "    <td>Write \"Perception & Psychophysics\" summary</td>\n",
    "    <td>1h15</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>25.10.23</td>\n",
    "    <td>Watch \"Integral transforms & Spectral features\" video</td>\n",
    "    <td>1h45</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>26.10.23</td>\n",
    "    <td>Write \"Integral transforms & Spectral features\" summary</td>\n",
    "    <td>1h10</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>26.10.23</td>\n",
    "    <td>Watch \"Semantics\" video</td>\n",
    "    <td>55min</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>27.10.23</td>\n",
    "    <td>Write \"Semantics\" summary</td>\n",
    "    <td>40min</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>01.11.23</td>\n",
    "    <td>Watch \"Learning over time\" video</td>\n",
    "    <td>1h50</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>02.11.23</td>\n",
    "    <td>Write \"Learning over time\" summary</td>\n",
    "    <td>1h20</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "**Alina Ehart:**\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>Date</th>\n",
    "    <th>Task</th>\n",
    "    <th>Hours</th>\n",
    "\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>24.10.23</td>\n",
    "    <td>Watch video 1 + Abstract</td>\n",
    "    <td>3h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>31.10.23</td>\n",
    "    <td>Watch video 2 + Note Taking</td>\n",
    "    <td>2h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3.11.23</td>\n",
    "    <td>Watch Writing abstract 2</td>\n",
    "    <td>1h30</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10.11.23</td>\n",
    "    <td>Watch video 3 + Note taking + Abstract 3</td>\n",
    "    <td>4h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>04.11.23</td>\n",
    "    <td>Watch Video 4 & 5 + Note Taking</td>\n",
    "    <td>2h 30</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>12.11.23</td>\n",
    "    <td>Writing abstract 4</td>\n",
    "    <td>1h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>13.11.23</td>\n",
    "    <td>Writing abstract 5</td>\n",
    "    <td>1h30</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>14.11.23</td>\n",
    "    <td>Watch video 6</td>\n",
    "    <td>2h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>15.11.23</td>\n",
    "    <td>Writing abstract 6</td>\n",
    "    <td>1h30</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>16.11.23</td>\n",
    "    <td>Watch video 7 + Note Taking</td>\n",
    "    <td>2h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>17.11.23</td>\n",
    "    <td>Writing abstract 7</td>\n",
    "    <td>2h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>18.11.23</td>\n",
    "    <td>Watch video 8 + Writing Abstract 8</td>\n",
    "    <td>2h30</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>19.11.23</td>\n",
    "    <td>Watch video 9 + Writing Abstract 9 + Revising Abstracts Document</td>\n",
    "    <td>5h</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our general approach to the assignment and work distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For or work organization, as both of us work on both Similarity Modeling 1 and 2, we decided to split the work on each course by modality. For example, for Similarity Modeling 1, Alina would work on visual features and Daniel on audio features, and we would sit down together to combine them in a hybrid classifier as well afterwards. To ensure that we both work thoroughly on each modality, the person who did audio on one course did visual on the other one (Daniel on SM1 audio and SM2 visual, Alina on SM1 visual and SM2 audio).  \n",
    "Before starting that work, we also sat down together to brainstorm and decide on our general approaches: we looked through the data and discussed what kind of aspects are representative of each character and how we could extract them. Based on this, we decided on the feature extractors and classifiers we would want to try to ensure that we use different ones.\n",
    "\n",
    "For our experiments, a generic data handling class has been implemented (`MuppetDataset.py`) where audio and image extraction, as well as loading, is abstracted. This class is then used in the respective notebooks for the experiments, where feature extraction methods have then been applied on the data contained by the class to generate tabular-form data that also contains the provided ground truth. That tabular data has been used in the specific classifiers, as well as exported and merged for the hybrid classifier.The approaches, experiments and results for each modality are described in the respective sections here, and even more in detail in their notebooks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio-based detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision-based detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result discussion, observations and other things we tried to improve performance\n",
    "\n",
    "For the vision-based classifier, frames of the video have been extracted using Open-CV at 25 frames per second. These have been stored on disk, generating more than 60 GB of data that had to be processed at feature extraction time. The features have been extracted and stored in a tabular format. The dataset has been split into training and testing sets using a 80/20 split, and the classifiers have been trained on the training set and evaluated on the testing set.\n",
    "\n",
    "The performance of the model has been evaluated under three angles:\n",
    "\n",
    "- Pigs classification (_metrics/plots for success at classifying frames where the Pigs flag is true_).\n",
    "- Swedish Chef classification (_metrics/plots for success at classifying frames where the Swedish chef flag is true_).\n",
    "- Overall classification performance (_metrics/plots for the general multi-class classification task - weighted performance metrics have been employed_).\n",
    "\n",
    "The best performing model uses the following features (_explanations on why we chose them for characteristics of the characters are given in the feature extraction part of the notebook_):\n",
    "\n",
    "- **DAISY descriptors**: 12 descriptors per frame, flattened into a 1D array of length $3 \\times 4 \\times 66 = 792$. These features describe local regions of the image and are invariant to rotation and scale through the use of circularly symmetric kernels. They are extracted in grayscale, and mainly were chosen as a way to detect specific shapes (e.g. the discriminative pigs' snouts where the pink color would not suffice).\n",
    "- **Lucas-Kanade optical flow**: 10 features per frame. These features describe the motion of specific features in the image. For each frame, we extract the descriptors and use them to compute velocity and displacement, and compute different statistical moments of those properties. The descriptors are extracted in grayscale, and were chosen as a way to detect the motion of the Swedish chef, which is a very distinct characteristic of his appearances. The Lucas-Kanade method is a sparse optical flow technique that estimates the motion of specific features in the image. It does so by assuming that the motion of these features is constant in a small neighborhood of the pixel. It then estimates the motion of these features by solving a least-squares problem.\n",
    "- **Farneback optical flow**: This algorithm was tested as an alternative/complement to the Lucas-Kanade algorithm. The extracted features are different as they describe the motion of every pixel in the image in this case (_dense vs. sparse optical flow_). For each frame, we extract the descriptors and this time compute statistical moments of the flow, but also derive histograms of the angle and magnitude of displacement. Another derived feature is that we divide the image into a grid, and compute similar aggregates for each cell of that grid (square in the image) for more precise regional descriptors.\n",
    "\n",
    "These features are fed, as a vector of values, into a random forest classifier. We chose this specific model due to the daisy feature extractor that leads to a large feature vector. Random forests are known to perform well on high-dimensional data, and are also pretty robust to overfitting (in comparison to non-ensemble methods). This is why we use this model here, even if it was talked about in the Sim1 lecture. To ensure that we respect the demand for 3 \"complex\" methods from Sim2, we used a SVM classifier (from Sim2) in the video-based model of Sim1 to compensate for the use of a random forest here. The model hyperparameters have been tuned through grid search.\n",
    "\n",
    "The model has been evaluated on a random split of the dataset, where 80% of the frames are used for training and 20% for testing. The evaluation has been performed for the non-random split implemented and described in Sim 1 Audio as well, but as those two approaches have been discussed and compared extensively in that notebook, we will focus on the random split here. The metrics and curves for the non-random split are available in `results/sim2_visual`.  \n",
    "In this setup, the best performing model, using a combination of all features described above, achieves the following performance metrics:\n",
    "\n",
    "**Performance all three feature-extractors:**\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th><b>Classification task</b></th>\n",
    "    <th><b>F1</b></th>\n",
    "    <th><b>Accuracy</b></th>\n",
    "    <th><b>Precision</b></th>\n",
    "    <th><b>Recall</b></th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td><b>Pigs detection</b></td>\n",
    "    <td>0.9849</td>\n",
    "    <td>0.0.9944</td>\n",
    "    <td>0.9948</td>\n",
    "    <td>0.9751</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Swedish Chef detection</b></td>\n",
    "    <td>0.9804</td>\n",
    "    <td>0.999</td>\n",
    "    <td>0.9983</td>\n",
    "    <td>0.963</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Overall multi-class classification</b></td>\n",
    "    <td>0.9934</td>\n",
    "    <td>0.9934</td>\n",
    "    <td>0.9935</td>\n",
    "    <td>0.9934</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<img src=\"../results/sim2_visual/all_randomsplit.png\" style=\"max-width:75%; display: block; margin: 0 auto\">\n",
    "\n",
    "In comparison, this is the performance with only the daisy features.  \n",
    "**Performance with the non-random split:**\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th><b>Classification task</b></th>\n",
    "    <th><b>F1</b></th>\n",
    "    <th><b>Accuracy</b></th>\n",
    "    <th><b>Precision</b></th>\n",
    "    <th><b>Recall</b></th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td><b>Pigs detection</b></td>\n",
    "    <td>0.9777</td>\n",
    "    <td>0.9918</td>\n",
    "    <td>0.9952</td>\n",
    "    <td>0.9609</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Swedish Chef detection</b></td>\n",
    "    <td>0.9828</td>\n",
    "    <td>0.9991</td>\n",
    "    <td>1</td>\n",
    "    <td>0.9662</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Overall multi-class classification</b></td>\n",
    "    <td>0.9909</td>\n",
    "    <td>0.9909</td>\n",
    "    <td>0.9909</td>\n",
    "    <td>0.9909</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<img src=\"../results/sim2_visual/daisy_randomsplit.png\" style=\"max-width:75%; display: block; margin: 0 auto\">\n",
    "\n",
    "Similarly, this is the performance with only the two motion-based feature-extractors.  \n",
    "**Performance with Lucas-Kanade and Farneback optical flow:**\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th><b>Classification task</b></th>\n",
    "    <th><b>F1</b></th>\n",
    "    <th><b>Accuracy</b></th>\n",
    "    <th><b>Precision</b></th>\n",
    "    <th><b>Recall</b></th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td><b>Pigs detection</b></td>\n",
    "    <td>0.5244</td>\n",
    "    <td>0.8785</td>\n",
    "    <td>0.9829</td>\n",
    "    <td>0.3576</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Swedish Chef detection</b></td>\n",
    "    <td>0.4313</td>\n",
    "    <td>0.9805</td>\n",
    "    <td>1</td>\n",
    "    <td>0.2749</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Overall multi-class classification</b></td>\n",
    "    <td>0.8311</td>\n",
    "    <td>0.8593</td>\n",
    "    <td>0.8782</td>\n",
    "    <td>0.8593</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<img src=\"../results/sim2_visual/lk_fb_randomsplit.png\" style=\"max-width:75%; display: block; margin: 0 auto\">\n",
    "\n",
    "**These different kinds of features, as well as their combinations, are the result of many experiments to improve the final performance.** For example, at the beginning, the entire extracted features were fed before their aggregates. Some more advanced aggregates (the angle histograms, or the division of the frames into grids for which we extracted individual features) have also been introduced in the process of evaluating the performance with only the general motion-based features and then trying to improve that performance.\n",
    "\n",
    "The numbers and figures displayed above are the result of the best performance we achieved with each family of features. What we observe from our experiments is that the daisy features are the most discriminative, and that the motion-based features are not as useful for the classification task in general. This is probably due to the fact that the motion-based features are not very discriminative, and that the daisy features are already very good at classifying the videos. While adding the motion information to the daisy features does improve performance, the different is not dramatic (a few percents) because the performance was already very good. The motion features by themselves lead to a worse classifier, which was to a certain extent expected. However, we are surprised that this is also the case for the Swedish Chef detection task, where we expected the motion features to be more useful. This might be because our specific flow description technique does not capture the Swedish chef movement patterns very well, or because they are not sufficiently disciminative by themselves.\n",
    "\n",
    "During our experiments, we also experienced that classification performance is not the only thing that should be considered when choosing these techniques as their runtime can be significantly longer than for the audio feature extraction techniques we tried. Indeed, the daisy features are extremely slow to extract, magnitudes longer than the motion features. Extracting them for the entire dataset took about 20 hours, while the motion features were extracted in about an hour. This made iterative improvement, debugging and experiments quite tedious. The daisy features are very powerful but cost a lot to compute, and we found that this tradeoff can be tuned by reducing the number of keypoints and/or the number of histograms. By doing so, performance decreases slightly while making extraction cheaper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid approach combining the features from both modalities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: explain how we combined the features + classifier we chose\n",
    "\n",
    "TODO: discuss results\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
