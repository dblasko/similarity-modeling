{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary for the Similarity Modeling 2 assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timesheets for the lectures and summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the time spent on the project, please refer to the specific notebooks. The following tables are dedicated to the time spent on watching the lectures and writing the summaries.\n",
    "\n",
    "**Daniel Blasko:**\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>Date</th>\n",
    "    <th>Task</th>\n",
    "    <th>Hours</th>\n",
    "\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>13.10.23</td>\n",
    "    <td>Watch \"Setting\" video</td>\n",
    "    <td>1h20</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>14.10.23</td>\n",
    "    <td>Write \"Setting\" summary</td>\n",
    "    <td>1h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>13.10.23</td>\n",
    "    <td>Watch \"Similarity Measurement\" video</td>\n",
    "    <td>1h10</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>14.10.23</td>\n",
    "    <td>Write \"Similarity Measurement\" summary</td>\n",
    "    <td>50min</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>14.10.23</td>\n",
    "    <td>Watch \"Feature engineering\" video</td>\n",
    "    <td>1h40</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>15.10.23</td>\n",
    "    <td>Write \"Feature engineering\" summary</td>\n",
    "    <td>1h10</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>18.10.23</td>\n",
    "    <td>Watch \"Classification\" video</td>\n",
    "    <td>1h50</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>19.10.23</td>\n",
    "    <td>Write \"Classification\" summary</td>\n",
    "    <td>1h20</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>18.10.23</td>\n",
    "    <td>Watch \"Evaluation\" video</td>\n",
    "    <td>45min</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>19.10.23</td>\n",
    "    <td>Write \"Evaluation\" summary</td>\n",
    "    <td>50min</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>19.10.23</td>\n",
    "    <td>Watch \"Perception & Psychophysics\" video</td>\n",
    "    <td>1h50</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>20.10.23</td>\n",
    "    <td>Write \"Perception & Psychophysics\" summary</td>\n",
    "    <td>1h15</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>25.10.23</td>\n",
    "    <td>Watch \"Integral transforms & Spectral features\" video</td>\n",
    "    <td>1h45</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>26.10.23</td>\n",
    "    <td>Write \"Integral transforms & Spectral features\" summary</td>\n",
    "    <td>1h10</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>26.10.23</td>\n",
    "    <td>Watch \"Semantics\" video</td>\n",
    "    <td>55min</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>27.10.23</td>\n",
    "    <td>Write \"Semantics\" summary</td>\n",
    "    <td>40min</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>01.11.23</td>\n",
    "    <td>Watch \"Learning over time\" video</td>\n",
    "    <td>1h50</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>02.11.23</td>\n",
    "    <td>Write \"Learning over time\" summary</td>\n",
    "    <td>1h20</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "**Alina Ehart:**\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>Date</th>\n",
    "    <th>Task</th>\n",
    "    <th>Hours</th>\n",
    "\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>24.10.23</td>\n",
    "    <td>Watch video 1 + Abstract</td>\n",
    "    <td>3h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>31.10.23</td>\n",
    "    <td>Watch video 2 + Note Taking</td>\n",
    "    <td>2h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3.11.23</td>\n",
    "    <td>Watch Writing abstract 2</td>\n",
    "    <td>1h30</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10.11.23</td>\n",
    "    <td>Watch video 3 + Note taking + Abstract 3</td>\n",
    "    <td>4h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>04.11.23</td>\n",
    "    <td>Watch Video 4 & 5 + Note Taking</td>\n",
    "    <td>2h 30</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>12.11.23</td>\n",
    "    <td>Writing abstract 4</td>\n",
    "    <td>1h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>13.11.23</td>\n",
    "    <td>Writing abstract 5</td>\n",
    "    <td>1h30</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>14.11.23</td>\n",
    "    <td>Watch video 6</td>\n",
    "    <td>2h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>15.11.23</td>\n",
    "    <td>Writing abstract 6</td>\n",
    "    <td>1h30</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>16.11.23</td>\n",
    "    <td>Watch video 7 + Note Taking</td>\n",
    "    <td>2h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>17.11.23</td>\n",
    "    <td>Writing abstract 7</td>\n",
    "    <td>2h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>18.11.23</td>\n",
    "    <td>Watch video 8 + Writing Abstract 8</td>\n",
    "    <td>2h30</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>19.11.23</td>\n",
    "    <td>Watch video 9 + Writing Abstract 9 + Revising Abstracts Document</td>\n",
    "    <td>5h</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our general approach to the assignment and work distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For or work organization, as both of us work on both Similarity Modeling 1 and 2, we decided to split the work on each course by modality. For example, for Similarity Modeling 1, Alina would work on visual features and Daniel on audio features, and we would sit down together to combine them in a hybrid classifier as well afterwards. To ensure that we both work thoroughly on each modality, the person who did audio on one course did visual on the other one (Daniel on SM1 audio and SM2 visual, Alina on SM1 visual and SM2 audio).  \n",
    "Before starting that work, we also sat down together to brainstorm and decide on our general approaches: we looked through the data and discussed what kind of aspects are representative of each character and how we could extract them. Based on this, we decided on the feature extractors and classifiers we would want to try to ensure that we use different ones.\n",
    "\n",
    "For our experiments, a generic data handling class has been implemented (`MuppetDataset.py`) where audio and image extraction, as well as loading, is abstracted. This class is then used in the respective notebooks for the experiments, where feature extraction methods have then been applied on the data contained by the class to generate tabular-form data that also contains the provided ground truth. That tabular data has been used in the specific classifiers, as well as exported and merged for the hybrid classifier.The approaches, experiments and results for each modality are described in the respective sections here, and even more in detail in their notebooks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision-based detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result discussion, observations and other things we tried to improve performance\n",
    "\n",
    "For the vision-based classifier, frames of the video have been extracted using Open-CV at 25 frames per second. These have been stored on disk, generating more than 60 GB of data that had to be processed at feature extraction time. The features have been extracted and stored in a tabular format. The dataset has been split into training and testing sets using a 80/20 split, and the classifiers have been trained on the training set and evaluated on the testing set.\n",
    "\n",
    "The performance of the model has been evaluated under three angles:\n",
    "\n",
    "- Pigs classification (_metrics/plots for success at classifying frames where the Pigs flag is true_).\n",
    "- Swedish Chef classification (_metrics/plots for success at classifying frames where the Swedish chef flag is true_).\n",
    "- Overall classification performance (_metrics/plots for the general multi-class classification task - weighted performance metrics have been employed_).\n",
    "\n",
    "The best performing model uses the following features (_explanations on why we chose them for characteristics of the characters are given in the feature extraction part of the notebook_):\n",
    "\n",
    "- **DAISY descriptors**: 12 descriptors per frame, flattened into a 1D array of length $3 \\times 4 \\times 66 = 792$. These features describe local regions of the image and are invariant to rotation and scale through the use of circularly symmetric kernels. They are extracted in grayscale, and mainly were chosen as a way to detect specific shapes (e.g. the discriminative pigs' snouts where the pink color would not suffice).\n",
    "- **Lucas-Kanade optical flow**: 10 features per frame. These features describe the motion of specific features in the image. For each frame, we extract the descriptors and use them to compute velocity and displacement, and compute different statistical moments of those properties. The descriptors are extracted in grayscale, and were chosen as a way to detect the motion of the Swedish chef, which is a very distinct characteristic of his appearances. The Lucas-Kanade method is a sparse optical flow technique that estimates the motion of specific features in the image. It does so by assuming that the motion of these features is constant in a small neighborhood of the pixel. It then estimates the motion of these features by solving a least-squares problem.\n",
    "- **Farneback optical flow**: This algorithm was tested as an alternative/complement to the Lucas-Kanade algorithm. The extracted features are different as they describe the motion of every pixel in the image in this case (_dense vs. sparse optical flow_). For each frame, we extract the descriptors and this time compute statistical moments of the flow, but also derive histograms of the angle and magnitude of displacement. Another derived feature is that we divide the image into a grid, and compute similar aggregates for each cell of that grid (square in the image) for more precise regional descriptors.\n",
    "\n",
    "These features are fed, as a vector of values, into a random forest classifier. We chose this specific model due to the daisy feature extractor that leads to a large feature vector. Random forests are known to perform well on high-dimensional data, and are also pretty robust to overfitting (in comparison to non-ensemble methods). This is why we use this model here, even if it was talked about in the Sim1 lecture. To ensure that we respect the demand for 3 \"complex\" methods from Sim2, we used a SVM classifier (from Sim2) in the video-based model of Sim1 to compensate for the use of a random forest here. The model hyperparameters have been tuned through grid search.\n",
    "\n",
    "The model has been evaluated on a random split of the dataset, where 80% of the frames are used for training and 20% for testing. The evaluation has been performed for the non-random split implemented and described in Sim 1 Audio as well, but as those two approaches have been discussed and compared extensively in that notebook, we will focus on the random split here. The metrics and curves for the non-random split are available in `results/sim2_visual`.  \n",
    "In this setup, the best performing model, using a combination of all features described above, achieves the following performance metrics:\n",
    "\n",
    "**Performance all three feature-extractors:**\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th><b>Classification task</b></th>\n",
    "    <th><b>F1</b></th>\n",
    "    <th><b>Accuracy</b></th>\n",
    "    <th><b>Precision</b></th>\n",
    "    <th><b>Recall</b></th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td><b>Pigs detection</b></td>\n",
    "    <td>0.9849</td>\n",
    "    <td>0.0.9944</td>\n",
    "    <td>0.9948</td>\n",
    "    <td>0.9751</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Swedish Chef detection</b></td>\n",
    "    <td>0.9804</td>\n",
    "    <td>0.999</td>\n",
    "    <td>0.9983</td>\n",
    "    <td>0.963</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Overall multi-class classification</b></td>\n",
    "    <td>0.9934</td>\n",
    "    <td>0.9934</td>\n",
    "    <td>0.9935</td>\n",
    "    <td>0.9934</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<img src=\"../results/sim2_visual/all_randomsplit.png\" style=\"max-width:75%; display: block; margin: 0 auto\">\n",
    "\n",
    "In comparison, this is the performance with only the daisy features.  \n",
    "**Performance with the non-random split:**\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th><b>Classification task</b></th>\n",
    "    <th><b>F1</b></th>\n",
    "    <th><b>Accuracy</b></th>\n",
    "    <th><b>Precision</b></th>\n",
    "    <th><b>Recall</b></th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td><b>Pigs detection</b></td>\n",
    "    <td>0.9777</td>\n",
    "    <td>0.9918</td>\n",
    "    <td>0.9952</td>\n",
    "    <td>0.9609</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Swedish Chef detection</b></td>\n",
    "    <td>0.9828</td>\n",
    "    <td>0.9991</td>\n",
    "    <td>1</td>\n",
    "    <td>0.9662</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Overall multi-class classification</b></td>\n",
    "    <td>0.9909</td>\n",
    "    <td>0.9909</td>\n",
    "    <td>0.9909</td>\n",
    "    <td>0.9909</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<img src=\"../results/sim2_visual/daisy_randomsplit.png\" style=\"max-width:75%; display: block; margin: 0 auto\">\n",
    "\n",
    "Similarly, this is the performance with only the two motion-based feature-extractors.  \n",
    "**Performance with Lucas-Kanade and Farneback optical flow:**\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th><b>Classification task</b></th>\n",
    "    <th><b>F1</b></th>\n",
    "    <th><b>Accuracy</b></th>\n",
    "    <th><b>Precision</b></th>\n",
    "    <th><b>Recall</b></th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td><b>Pigs detection</b></td>\n",
    "    <td>0.5244</td>\n",
    "    <td>0.8785</td>\n",
    "    <td>0.9829</td>\n",
    "    <td>0.3576</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Swedish Chef detection</b></td>\n",
    "    <td>0.4313</td>\n",
    "    <td>0.9805</td>\n",
    "    <td>1</td>\n",
    "    <td>0.2749</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Overall multi-class classification</b></td>\n",
    "    <td>0.8311</td>\n",
    "    <td>0.8593</td>\n",
    "    <td>0.8782</td>\n",
    "    <td>0.8593</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<img src=\"../results/sim2_visual/lk_fb_randomsplit.png\" style=\"max-width:75%; display: block; margin: 0 auto\">\n",
    "\n",
    "**These different kinds of features, as well as their combinations, are the result of many experiments to improve the final performance.** For example, at the beginning, the entire extracted features were fed before their aggregates. Some more advanced aggregates (the angle histograms, or the division of the frames into grids for which we extracted individual features) have also been introduced in the process of evaluating the performance with only the general motion-based features and then trying to improve that performance.\n",
    "\n",
    "The numbers and figures displayed above are the result of the best performance we achieved with each family of features. What we observe from our experiments is that the daisy features are the most discriminative, and that the motion-based features are not as useful for the classification task in general. This is probably due to the fact that the motion-based features are not very discriminative, and that the daisy features are already very good at classifying the videos. While adding the motion information to the daisy features does improve performance, the different is not dramatic (a few percents) because the performance was already very good. The motion features by themselves lead to a worse classifier, which was to a certain extent expected. However, we are surprised that this is also the case for the Swedish Chef detection task, where we expected the motion features to be more useful. This might be because our specific flow description technique does not capture the Swedish chef movement patterns very well, or because they are not sufficiently disciminative by themselves.\n",
    "\n",
    "During our experiments, we also experienced that classification performance is not the only thing that should be considered when choosing these techniques as their runtime can be significantly longer than for the audio feature extraction techniques we tried. Indeed, the daisy features are extremely slow to extract, magnitudes longer than the motion features. Extracting them for the entire dataset took about 20 hours, while the motion features were extracted in about an hour. This made iterative improvement, debugging and experiments quite tedious. The daisy features are very powerful but cost a lot to compute, and we found that this tradeoff can be tuned by reducing the number of keypoints and/or the number of histograms. By doing so, performance decreases slightly while making extraction cheaper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio-based detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Discussion, observations and other things we tried to improve performance\n",
    "\n",
    "In this case again, the performance of the model has been evaluated under four angles:  \n",
    "Remark: Miss Piggy has been treated seperately as in doubt of her Auio_MissPiggy being a subset of Audio_Pigs.\n",
    "\n",
    "- Pigs_Audio classification (metrics/plots for success at classifying frames where Pigs_Audio flag is true)\n",
    "- Swedish Chef classification (metrics / plots for sucess at classifying frames where Audio_Cook flag is true)\n",
    "- Miss_Piggy_Audio classification (metrics for success at classifying frames where MissPiggy_Audio flag is true)\n",
    "- Overall classification performance (metrics/plots for the general multi-class classification task - weighted performance metrics have been employed).\n",
    "\n",
    "The best performing model uses the following features: - 19 MFCCs - 2 Poly Features - 5 Spectral_contrast - 6 Chroma - Sharpness - Spectral rolloff\n",
    "\n",
    "All these values are fed, as a vector of values, into a ridge classifier with single value decomposition solver and balanced weight. All parameters are normalized, to make sure they are all of equal importance in the distance metric that the classifier uses by constraining them to the same order of magnitude.\n",
    "This way, the best performing model, with the features described above, achieves the following performance metrics (in the random split, as we from Similarity Modelling 1 know that a random split leads to better performance):\n",
    "\n",
    "**Performance with the random split:**\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    " <tr>\n",
    " <th><b>Experiment</b></th>\n",
    " <th><b>F1 </b></th>\n",
    " <th><b>Accuracy</b></th>\n",
    " <th><b>Precision</b></th>\n",
    " <th><b>Recall</b></th>\n",
    " </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    " <tr>\n",
    " <td><b>Pigs detection</b></td>\n",
    " <td>0.17907</td>\n",
    " <td>0.6511</td>\n",
    " <td>0.1027</td>\n",
    " <td>0.6961</td>\n",
    " </tr>\n",
    " <tr>\n",
    " <td><b>Swedish Chef detection</b></td>\n",
    " <td>0.0866</td>\n",
    " <td>0.7689</td>\n",
    " <td>0.0458</td>\n",
    " <td>0.7912</td>\n",
    "\n",
    "<tr>\n",
    " <td><b>Miss Piggy detection</b></td>\n",
    " <td>0.1452</td>\n",
    " <td>0.6851</td>\n",
    " <td>0.0807</td>\n",
    " <td>0.7243</td>\n",
    "\n",
    " </tr>\n",
    " <tr>\n",
    " <td><b>Overall multi-class classification</b></td>\n",
    " <td>0.5766</td>\n",
    " <td>0.4449</td>\n",
    " <td>0.9120</td>\n",
    " <td>0.4449</td>\n",
    " </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<img src=\"../results/sim2_audio/sm2_pigs_best.png\" style=\"max-width:75%; display: block; margin: 0 auto\">\n",
    "\n",
    "The general performance we have achieved is not as good as we would wish, but it still can bring valuable information to the hybrid classifier that also uses visual information - detecting solely on audio is not as easy.  \n",
    "We observed that there are no signficant performance differences for the three models and all characters seem to have a similar presence rate. As they are all not present very often, the recall is signficantly higher than the precision and the accuracy does not look too bad either. This performance is reflected in the ROC and precision-recall curves. The ROC curve does not look too bad and indicates that the models are better than random guessing (that would be 0.5, but we do be having: pigs: 0.67, cook: 0.78 and miss piggy: 0.7) The preciscion-recall curves, however, look pretty poor for all three models (close to zero). The confusion matrices show that all three models do be having a tendency towards false positives rather than false negatives. This may be due to class imbalances (more true negatives than true positives). Data quality and feature representation contribute to the poor performance as well. With our given resources (Laptops) it was not easy to extract such large amounts of features in short time. Further, we unfortunately were unable to process several enriching audio features to our needs (dataframe level with sampling rate 25 frames/s, issues describe in nb previously). Despite trying to use differt features and feature combinations, we also tried different classifier parameters and even classifiers to achieve better performance - unfortunately in vain.  \n",
    "The overall performance seems a little better, except for accuracy, all other measures indicate better performance. This may be due the majority of frames featuring neither of the characters.\n",
    "\n",
    "Even though the above performance is the best we achieved, many approaches have been attempted tried to improve the performance of the model as discussed in the audio notebook.\n",
    "\n",
    "- We tried using poly_features & mel_centroid only as these features have not been used in SM1. This did not work at all, we could not predict a single character presence. All measures, despite accuracy (quite ok as the characters are not present in most frames), were 0.\n",
    "\n",
    "  - tried with ridge classifier, auto solver, no weight adjustment\n",
    "  - tried with svm, rbf kernel, to see whether it was a classifier issue - it was not\n",
    "  - same terrible result\n",
    "  - for pigs for example, the performance was terrible\n",
    "    Accuracy: 0.9453337360314105\n",
    "    Precision: 0.0\n",
    "    Recall: 0.0\n",
    "    F1 Score: 0.0\n",
    "    Confusion Matrix:\n",
    "    [[21910     0]\n",
    "[ 1267     0]]\n",
    "\n",
    "- We tried ridge classifier with poly features only, it performed very bad.\n",
    "\n",
    "  - for pigs: e.g.: a little better than random guessing\n",
    "    Accuracy: 0.4952323424084221\n",
    "    Precision: 0.06409827845562427\n",
    "    Recall: 0.6053670086819258\n",
    "    F1 Score: 0.11592231542356231\n",
    "    Confusion Matrix:\n",
    "    [[10711 11199]\n",
    "[  500   767]]\n",
    "\n",
    "- for cook: e.g.: very close to random guessing\n",
    "  Accuracy: 0.5637053975924408\n",
    "  Precision: 0.014768559817623154\n",
    "  Recall: 0.46417445482866043\n",
    "  F1 Score: 0.028626320845341016\n",
    "  Confusion Matrix:\n",
    "  [[12916  9940]\n",
    "                  [  172   149]]\n",
    "\n",
    "  - for miss piggy:e.g.: a little better than random guessing\n",
    "    Accuracy: 0.5052422660396082\n",
    "    Precision: 0.046344591705857204\n",
    "    Recall: 0.633177570093458\n",
    "    F1 Score: 0.08636762010995139\n",
    "    Confusion Matrix:\n",
    "    [[11168 11153]\n",
    "[  314   542]]\n",
    "\n",
    "- We tried ridge classifier with all features from SM1 - very similar performance, almost the same for pigs, a little better for miss piggy, a little bit worse for the swedish chef\n",
    "\n",
    "  - pigs:\n",
    "    Accuracy: 0.6688095957198947\n",
    "    Precision: 0.10782034022763431\n",
    "    Recall: 0.6953433307024467\n",
    "    F1 Score: 0.18669209578300489\n",
    "    Confusion Matrix:\n",
    "    [[14620  7290]\n",
    "[  386   881]]\n",
    "  - cook:\n",
    "    Accuracy: 0.7755964965267291\n",
    "    Precision: 0.04663693794128577\n",
    "    Recall: 0.7819314641744548\n",
    "    F1 Score: 0.08802384709801858\n",
    "    Confusion Matrix:\n",
    "    [[17725  5131]\n",
    "[   70   251]]\n",
    "  - miss piggy:\n",
    "    Accuracy: 0.7050955688829443\n",
    "    Precision: 0.08748447633503519\n",
    "    Recall: 0.7406542056074766\n",
    "    F1 Score: 0.15648525237566333\n",
    "    Confusion Matrix:\n",
    "    [[15708  6613]\n",
    "[  222   634]]\n",
    "\n",
    "- We also tried different solvers for the ridge classifier\n",
    "  e.g.: for pigs solvers: cholesky, sparse_cg, sag, saga returned the same metrics of:\n",
    "  Accuracy: 0.6688095957198947\n",
    "  Precision: 0.10782034022763431\n",
    "  Recall: 0.6953433307024467\n",
    "  F1 Score: 0.18669209578300489\n",
    "  Confusion Matrix:\n",
    "  [[14620  7290]\n",
    "[  386   881]]\n",
    "\n",
    "  - lsqr - minimally worse:\n",
    "    Metrics for Audio_Pigs ridge classifier, balanced, lsqr:\n",
    "    Accuracy: 0.6686370108297018\n",
    "    Precision: 0.10767160161507403\n",
    "    Recall: 0.6945540647198106\n",
    "    F1 Score: 0.18644067796610167\n",
    "    Confusion Matrix:\n",
    "    [[14617  7293]\n",
    "[  387   880]]\n",
    "\n",
    "  - lbfgs - siginificantly worse:\n",
    "    Accuracy: 0.60072485653881\n",
    "    Precision: 0.0794102159031069\n",
    "    Recall: 0.5951065509076559\n",
    "    F1 Score: 0.14012265378182492\n",
    "    Confusion Matrix:\n",
    "    [[13169  8741]\n",
    "[  513   754]]\n",
    "\n",
    "- We also tried with svm classifer with sigmoid kernel in an effort to improve bad performance: worked significantly worse, especially for swedish chef - only one true positive! and miss piggy only 3 true positives.\n",
    "\n",
    "  - pigs:\n",
    "    Accuracy: 0.9004185183587177\n",
    "    Precision: 0.06516290726817042\n",
    "    Recall: 0.06156274664561957\n",
    "    F1 Score: 0.06331168831168832\n",
    "    Confusion Matrix:\n",
    "    [[20791  1119]\n",
    "[ 1189    78]]\n",
    "\n",
    "  - cook:\n",
    "    Accuracy: 0.9799801527376278\n",
    "    Precision: 0.006896551724137931\n",
    "    Recall: 0.003115264797507788\n",
    "    F1 Score: 0.004291845493562232\n",
    "    Confusion Matrix:\n",
    "    [[22712 144]\n",
    "\n",
    "[ 320 1]]\n",
    "\n",
    "- miss piggy:\n",
    "  Accuracy: 0.9570695085645252\n",
    "  Precision: 0.020689655172413793\n",
    "  Recall: 0.0035046728971962616\n",
    "  F1 Score: 0.005994005994005994\n",
    "  Confusion Matrix:\n",
    "  [[22179 142]\n",
    "\n",
    "[ 853 3]]\n",
    "\n",
    "- mfcc & contrast: nearly the same as best\n",
    "  e.g.: pigs\n",
    "  Accuracy: 0.651076498252578\n",
    "  Precision: 0.1027493010251631\n",
    "  Recall: 0.6961325966850829\n",
    "  F1 Score: 0.17906811491219166\n",
    "  Confusion Matrix:\n",
    "  [[14208  7702]\n",
    "[  385   882]]\n",
    "\n",
    "- mfcc & chroma: little bit worse\n",
    "  e.g. pigs:\n",
    "  Accuracy: 0.6600077663200586\n",
    "  Precision: 0.10472205618649133\n",
    "  Recall: 0.691397000789266\n",
    "  F1 Score: 0.18189368770764117\n",
    "  Confusion Matrix:\n",
    "  [[14421  7489]\n",
    "[  391   876]]\n",
    "- poly feautures & sharpness: pretty bad\n",
    "  e.g.: pigs\n",
    "  Accuracy: 0.4950597575182293\n",
    "  Precision: 0.06407685881370091\n",
    "  Recall: 0.6053670086819258\n",
    "  F1 Score: 0.11588728563873989\n",
    "  Confusion Matrix:\n",
    "  [[10707 11203]\n",
    "[  500   767]]\n",
    "\n",
    "- mfcc & chroma & sharpness: pretty bad\n",
    "  e.g.: pigs\n",
    "  Accuracy: 0.4950597575182293\n",
    "  Precision: 0.06407685881370091\n",
    "  Recall: 0.6053670086819258\n",
    "  F1 Score: 0.11588728563873989\n",
    "  Confusion Matrix:\n",
    "  [[10707 11203]\n",
    "[  500   767]]\n",
    "\n",
    "- mfcc & poly features\n",
    "  e.g.: for pigs very similar to all features\n",
    "  Accuracy: 0.6501704275790654\n",
    "  Precision: 0.10305210630149704\n",
    "  Recall: 0.7008681925808997\n",
    "  F1 Score: 0.17968433832456493\n",
    "  Confusion Matrix:\n",
    "  [[14181  7729]\n",
    "[  379   888]]\n",
    "\n",
    "  e.g: for cook very similar all features\n",
    "  Accuracy: 0.7631272382102947\n",
    "  Precision: 0.04490227152667723\n",
    "  Recall: 0.794392523364486\n",
    "  F1 Score: 0.08499999999999999\n",
    "  Confusion Matrix:\n",
    "  [[17432  5424]\n",
    "[   66   255]]\n",
    "\n",
    "  e.g: for miss piggy very similar to all features\n",
    "  Accuracy: 0.6673857703758036\n",
    "  Precision: 0.07535010534143016\n",
    "  Recall: 0.7102803738317757\n",
    "  F1 Score: 0.13624649859943977\n",
    "  Confusion Matrix:\n",
    "  [[14860  7461]\n",
    "[  248   608]]\n",
    "\n",
    "All in all these further experiments were not successful at improved performance, but illustrate how we really fought through different ways (improving the classifier and the feature extractors) to try to fix this performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid approach combining the features from both modalities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the hybrid approach, we again combined the audio and visual features for each frame and trained a quadratic discriminant classifier on them. Gaussian mixture models were also implemented and tried, but final evaluation is done with the former due to performance limitations.  \n",
    "In the audio/visual notebooks, the extracted features were stored in dataframes that were exported as csv files. We loaded these csv files and merged them into one dataframe for the hybrid classifier.  \n",
    "We then randomly split the data into training and test sets (80/20%) and aggregated the labels for each frame by merging the visual and audio flags into a single one (overall presence of the character).\n",
    "\n",
    "The final results of the hybrid classifier are shown in the first table below. The performance of the hybrid classifier is not ideal: while we expected the audio features to not improve the performance much in comparison to the visual classifier (which was already very good, and audio only performed poorly), we did not initially expect the global performance to be worse than with visual features only. The model has a tendency to over-predict positives for both classes, especially the Swedish Chef (high recall and low precision). We think there are two potential reasons for this: we use a different classifier than in the visual notebook, which might deal with this high-dimensional data worse, and/or the model learns from noise in the less-useful audio features.  \n",
    "With further experiments, we found that it is a mixture of both hypotheses. As shown in the second table and figure below (_performance of the same classifier, but using the visual features only_), running the same model with only the visual data does improve performance (_so the added audio features turn out to be detrimental in this setup_), but it still performs worse than the classifier trained in the visual notebook (_the QDA classifier used here might be less powerful for this high-dimensional setup_).\n",
    "\n",
    "Scaling the input significantly boosted performance here, most probably due to the different scales between visual and audio features. Hyperparameter tuning (here tuning the regularization parameter) was also important: best performance is obtained with no regularization at all, and even then, the model still is not ideal. We hypothethize that this is due to the large amount of features that the model does not handle perfectly and it is not powerful enough to overfit on this high-dimensional data.\n",
    "\n",
    "**Performance of the hybrid classifier:**\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th><b>Experiment</b></th>\n",
    "    <th><b>F1</b></th>\n",
    "    <th><b>Accuracy</b></th>\n",
    "    <th><b>Precision</b></th>\n",
    "    <th><b>Recall</b></th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td><b>Pigs detection</b></td>\n",
    "    <td>0.5015</td>\n",
    "    <td>0.7704</td>\n",
    "    <td>0.4249</td>\n",
    "    <td>0.6119</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Swedish Chef detection</b></td>\n",
    "    <td>0.1413</td>\n",
    "    <td>0.6978</td>\n",
    "    <td>0.0765</td>\n",
    "    <td>0.926</td>\n",
    "\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Overall multi-class classification</b></td>\n",
    "    <td>0.5861</td>\n",
    "    <td>0.5095</td>\n",
    "    <td>0.8005</td>\n",
    "    <td>0.5095</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<img src=\"../results/sim2_hybrid/results.png\" style=\"max-width:75%; display: block; margin: 0 auto\">\n",
    "\n",
    "**Performance of the same hybrid classifier using only the visual features:**\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th><b>Experiment</b></th>\n",
    "    <th><b>F1</b></th>\n",
    "    <th><b>Accuracy</b></th>\n",
    "    <th><b>Precision</b></th>\n",
    "    <th><b>Recall</b></th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td><b>Pigs detection</b></td>\n",
    "    <td>0.7143</td>\n",
    "    <td>0.8681</td>\n",
    "    <td>0.6011</td>\n",
    "    <td>0.8800</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Swedish Chef detection</b></td>\n",
    "    <td>0.9627</td>\n",
    "    <td>0.9981</td>\n",
    "    <td>0.9932</td>\n",
    "    <td>0.9341</td>\n",
    "\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Overall multi-class classification</b></td>\n",
    "    <td>0.8752</td>\n",
    "    <td>0.8666</td>\n",
    "    <td>0.8984</td>\n",
    "    <td>0.8666</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<img src=\"../results/sim2_hybrid/qda_visual_randomsplit.png\" style=\"max-width:75%; display: block; margin: 0 auto\">\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
