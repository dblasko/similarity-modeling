{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary for the Similarity Modeling 1 assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timesheets for the lectures and summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the time spent on the project, please refer to the specific notebooks. The following tables are dedicated to the time spent on watching the lectures and writing the summaries.\n",
    "\n",
    "**Daniel Blasko:**\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>Date</th>\n",
    "    <th>Task</th>\n",
    "    <th>Hours</th>\n",
    "\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>13.10.23</td>\n",
    "    <td>Watch \"Setting\" video</td>\n",
    "    <td>1h20</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>14.10.23</td>\n",
    "    <td>Write \"Setting\" summary</td>\n",
    "    <td>1h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>13.10.23</td>\n",
    "    <td>Watch \"Similarity Measurement\" video</td>\n",
    "    <td>1h10</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>14.10.23</td>\n",
    "    <td>Write \"Similarity Measurement\" summary</td>\n",
    "    <td>50min</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>14.10.23</td>\n",
    "    <td>Watch \"Feature engineering\" video</td>\n",
    "    <td>1h40</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>15.10.23</td>\n",
    "    <td>Write \"Feature engineering\" summary</td>\n",
    "    <td>1h10</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>18.10.23</td>\n",
    "    <td>Watch \"Classification\" video</td>\n",
    "    <td>1h50</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>19.10.23</td>\n",
    "    <td>Write \"Classification\" summary</td>\n",
    "    <td>1h20</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>18.10.23</td>\n",
    "    <td>Watch \"Evaluation\" video</td>\n",
    "    <td>45min</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>19.10.23</td>\n",
    "    <td>Write \"Evaluation\" summary</td>\n",
    "    <td>50min</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>19.10.23</td>\n",
    "    <td>Watch \"Perception & Psychophysics\" video</td>\n",
    "    <td>1h50</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>20.10.23</td>\n",
    "    <td>Write \"Perception & Psychophysics\" summary</td>\n",
    "    <td>1h15</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>25.10.23</td>\n",
    "    <td>Watch \"Integral transforms & Spectral features\" video</td>\n",
    "    <td>1h45</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>26.10.23</td>\n",
    "    <td>Write \"Integral transforms & Spectral features\" summary</td>\n",
    "    <td>1h10</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>26.10.23</td>\n",
    "    <td>Watch \"Semantics\" video</td>\n",
    "    <td>55min</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>27.10.23</td>\n",
    "    <td>Write \"Semantics\" summary</td>\n",
    "    <td>40min</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>01.11.23</td>\n",
    "    <td>Watch \"Learning over time\" video</td>\n",
    "    <td>1h50</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>02.11.23</td>\n",
    "    <td>Write \"Learning over time\" summary</td>\n",
    "    <td>1h20</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "**Alina Ehart:**\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>Date</th>\n",
    "    <th>Task</th>\n",
    "    <th>Hours</th>\n",
    "\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>24.10.23</td>\n",
    "    <td>Watch video 1 + Abstract</td>\n",
    "    <td>3h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>31.10.23</td>\n",
    "    <td>Watch video 2 + Note Taking</td>\n",
    "    <td>2h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3.11.23</td>\n",
    "    <td>Watch Writing abstract 2</td>\n",
    "    <td>1h30</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10.11.23</td>\n",
    "    <td>Watch video 3 + Note taking + Abstract 3</td>\n",
    "    <td>4h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>04.11.23</td>\n",
    "    <td>Watch Video 4 & 5 + Note Taking</td>\n",
    "    <td>2h 30</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>12.11.23</td>\n",
    "    <td>Writing abstract 4</td>\n",
    "    <td>1h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>13.11.23</td>\n",
    "    <td>Writing abstract 5</td>\n",
    "    <td>1h30</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>14.11.23</td>\n",
    "    <td>Watch video 6</td>\n",
    "    <td>2h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>15.11.23</td>\n",
    "    <td>Writing abstract 6</td>\n",
    "    <td>1h30</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>16.11.23</td>\n",
    "    <td>Watch video 7 + Note Taking</td>\n",
    "    <td>2h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>17.11.23</td>\n",
    "    <td>Writing abstract 7</td>\n",
    "    <td>2h</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>18.11.23</td>\n",
    "    <td>Watch video 8 + Writing Abstract 8</td>\n",
    "    <td>2h30</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>19.11.23</td>\n",
    "    <td>Watch video 9 + Writing Abstract 9 + Revising Abstracts Document</td>\n",
    "    <td>5h</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our general approach to the assignment and work distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For or work organization, as both of us work on both Similarity Modeling 1 and 2, we decided to split the work on each course by modality. For example, for Similarity Modeling 1, Alina would work on visual features and Daniel on audio features, and we would sit down together to combine them in a hybrid classifier as well afterwards. To ensure that we both work thoroughly on each modality, the person who did audio on one course did visual on the other one (Daniel on SM1 audio and SM2 visual, Alina on SM1 visual and SM2 audio).  \n",
    "Before starting that work, we also sat down together to brainstorm and decide on our general approaches: we looked through the data and discussed what kind of aspects are representative of each character and how we could extract them. Based on this, we decided on the feature extractors and classifiers we would want to try to ensure that we use different ones.\n",
    "\n",
    "For our experiments, a generic data handling class has been implemented (`MuppetDataset.py`) where audio and image extraction, as well as loading, is abstracted. This class is then used in the respective notebooks for the experiments, where feature extraction methods have then been applied on the data contained by the class to generate tabular-form data that also contains the provided ground truth. That tabular data has been used in the specific classifiers, as well as exported and merged for the hybrid classifier.The approaches, experiments and results for each modality are described in the respective sections here, and even more in detail in their notebooks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio-based detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the audio-based classifier, to extract features, we mostly used the `librosa` library. However, before that, the data had to be preparated. As the annotations are on the frame-level, the audio features had to be aligned with the framerate. As there are 25 frames per second, and 16k audio samples per second, we have $\\frac{44100}{25} = 1764$ audio samples per frame and divided our audio features in windows of 1764 samples.\n",
    "\n",
    "As Kermit displays a distinct audio pattern where his interventions start with screaming, and transition to mumbling as he speaks, which should correspond to a high foundational frequency, we decided to extract the fundational frequency of the audio (pitch), as well as loudness. As for Waldorf & Statler, we observe they have low, cranky voices with a very specific overtone structure.  \n",
    "We therefore decided to extract spectral and timber features. However, as this will be discussed further later, many features we tried were not useful and lead to very bad classifiers (discussed further in the audio notebook). After different experiments, we have kept the MFCCs, that do describe the timbre of the sound by representing the short-term power spectrum of the signal. Those led to the best performance in our experiments.  \n",
    "All used features were then normalized and merged in a single dataframe along with the ground truth. We then split our data (both a random and non-random split in 80/20 proportions, experiments with both are discussed below) and used it to train a k-nearest-neighbors classifier.\n",
    "\n",
    "The performance of the model has been evaluated under three angles:\n",
    "\n",
    "- Kermit classification (_metrics/plots for success at classifying frames where the Kermit flag is true_). **_Reminder_**: as stated previously, for Kermit, there are no \"audio-only\" annotations, there might be undetected true positives/true negatives in the Kermit classification as the model might predict the absence of Kermit due to him not making sounds in the frame, but he might be present in the video making the label 1.\n",
    "- Waldorf & Statler classification (_metrics/plots for success at classifying frames where the Waldorf & Statler audio flag is true_).\n",
    "- Overall classification performance (_metrics/plots for the general multi-class classification task - weighted performance metrics have been employed_).\n",
    "\n",
    "The best performing model uses the following features (_explanations on why we chose them for characteristics of the characters are given in the feature extraction part of the notebook_):\n",
    "\n",
    "- Loudness (through RMS energy).\n",
    "- Zero-crossing rate.\n",
    "- 19 MFCCs to describe the timbre/the short-term power spectrum of the sound.\n",
    "\n",
    "All these features are fed, as a vector of values, into a k-nearest-neighbors classifier. All parameters are normalized, to make sure they are all of equal importance in the distance metric that the classifier uses by constraining them to the same order of magnitude. The number of neighbors considered, as well as the distance metric, are selected based on 5-fold cross-validation on the training dataset.  \n",
    "This way, the best performing model, with the features described above, achieves the following performance metrics (we report them for both the random and non-random split of the dataset described in the data splitting part of the notebook):\n",
    "\n",
    "**Performance with the random split:**\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th><b>Experiment</b></th>\n",
    "    <th><b>F1 (Kermit)</b></th>\n",
    "    <th><b>Accuracy</b></th>\n",
    "    <th><b>Precision</b></th>\n",
    "    <th><b>Recall</b></th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td><b>Kermit detection</b></td>\n",
    "    <td>0.5738</td>\n",
    "    <td>0.7894</td>\n",
    "    <td>0.6759</td>\n",
    "    <td>0.4985</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Waldorf & Statler detection</b></td>\n",
    "    <td>0.2549</td>\n",
    "    <td>0.9788</td>\n",
    "    <td>0.6131</td>\n",
    "    <td>0.1609</td>\n",
    "\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Overall multi-class classification</b></td>\n",
    "    <td>0.7579</td>\n",
    "    <td>0.7732</td>\n",
    "    <td>0.7608</td>\n",
    "    <td>0.7732</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<img src=\"../results/sim1_audio/rms_zcr_mfcc19__randomsplit.png\" style=\"max-width:75%; display: block; margin: 0 auto\">\n",
    "\n",
    "**Performance with the non-random split:**\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th><b>Classification task</b></th>\n",
    "    <th><b>F1</b></th>\n",
    "    <th><b>Accuracy</b></th>\n",
    "    <th><b>Precision</b></th>\n",
    "    <th><b>Recall</b></th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td><b>Kermit detection</b></td>\n",
    "    <td>0.31</td>\n",
    "    <td>0.5457</td>\n",
    "    <td>0.4746</td>\n",
    "    <td>0.2301</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Waldorf & Statler detection</b></td>\n",
    "    <td>0.0837</td>\n",
    "    <td>0.9839</td>\n",
    "    <td>0.1735</td>\n",
    "    <td>0.0552</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Overall multi-class classification</b></td>\n",
    "    <td>0.4928</td>\n",
    "    <td>0.5345</td>\n",
    "    <td>0.5132</td>\n",
    "    <td>0.5345</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<img src=\"../results/sim1_audio/rms_zcr_mfcc19_orderedsplit.png\" style=\"max-width:75%; display: block; margin: 0 auto\">\n",
    "\n",
    "A clear difference can be observed between the performance metrics/graphs for the random and non-random split of the dataset. We suspected this might be due to multiple potential reasons. Neighboring frames are highly correlated, and we would have such neighboring frames in both the train and test sets if we used a random split, leading to \"artificially\" higher performance. Moreover, the non-random split cuts off the last part of the last video. That part contains extremely little samples for Waldorf & Statler, and might also present very specific configurations of the characters that are not present in the rest of the dataset as it could be a quite different scene, leading to a worse performance on evaluation. In the rest of this commentary, we will compare the performance metrics for the random split, but all experiments have been executed with both approaches implemented in this notebooks, and we include the measurements for both splits in our `results` folder.\n",
    "The general performance we have achieved is not as good as we would wish, but it still can bring valuable information to the hybrid classifier that also uses visual information - detecting solely on audio is not as easy. We observe that the model works significantly better for detecting Kermit, which we expected: his audio patterns are more specific and easier to detect than Waldorf & Statler's, and most importantly, he's way more present in the dataset, allowing the model to learn more about his audio patterns (Kermit is present in 6590 frames of the test set, against 522 for Waldorf & Statler out of 23177 total frames). For Kermit detection, recall is lower than other performance metrics, and as the confusion matrix confirms, false negatives are the biggest problem of the model. And this is even though we optimized the cross validation based on recall, as we saw that this is the main problem. This performance is reflected in the ROC and precision-recall curves that, while they are not totally bad (area of 0 for the precision-recall and a diagonal line for the ROC curve would be the worst possible), they are not as good as we would wish.  \n",
    "For Waldorf & Statler, the model performs significantly worse, but still somewhat better than random guessing (as can be seen on the ROC curve that is not exactly diagonal). Both curves are a lot closer to the worst possible than for Kermit, and this is reflected in the performance metrics and confusion matrix as well. The confusion matrix shows that the model has a tendency towards false negatives as well, which we mostly blame on the very low presence of the characters in the dataset (only 522 / 23177 frames for the test set). The performance is poor, and we tried multiple approaches to fix this as it will be described in the next section, but this is the best performance we got. Finally, the general performance metrics for the multi-class classification task are quite good, but this is only due to the high-presence of Kermit in the dataset, and that most frames of the dataset are of class \"no character\". As the model has a tendency towards false negatives, this explains the inflated average performance metrics, and underlines the importance on the per-character evaluation we did before.\n",
    "\n",
    "**Even though the above performance is the best we achieved, multiple approaches have been tried to improve the performance of the model.**\n",
    "\n",
    "- As we use a k-NN classifier, the many MFCC features might override the importance of ZCR/loudness. We tried to use statistical moments of the MFCCs (mean, standard deviation, skewness, kurtosis...) instead of the full representation to limit this, but this led to a (slightly) worse performance. We suspect that this is due to the fact that the full representation of the MFCCs is more expressive and allows the model to learn more about the audio patterns of the characters, and we have therefore kept the full representation.\n",
    "- We have experimented with different numbers of MFCC features as well. We have seen a significant increase in performance when scaling the number between 13 and 19, and after 19, the performance decreased. We have therefore kept 19 MFCCs.\n",
    "- In general, for the k-NN classifier, we have observed a precision/recall tradeoff based on the chosen value for `k`: lower values led to less false negatives while higher values led to higher precision.\n",
    "- We also experimented with multiple other audio feature-engineering methods specifically aimed at the specific patterns of Waldorf and Statler (_low, cranky voices with a very specific overtone structure, leading us to focus on spectral and timber features_) as those were our major problem.\n",
    "\n",
    "  - We tried extracting spectral contrast to measure the difference in amplitude between peaks and valleys in the spectrum, capturing some aspects of timbre.\n",
    "  - We also used spectral roll-off, providing insights into the shape of the spectral energy distribution, affecting the timbre.\n",
    "  - Spectral centroids have been tried to describe the brightness of the sound.\n",
    "  - Pitch has been extracted, and its statistical moments were used as features as well.\n",
    "  - We also gave chroma features a try in an effort to achieve better performance by capturing the energy distribution across different pitch classes to provide a view of the harmonic content that contributes to timbre.\n",
    "\n",
    "  However, none of these approaches led to an improvement in performance (whether by itself, or in combinations of features). For multiple features, we also observed in their statistical distribution that they were almost always of value almost 1 (once normalized), bringing little to no information to the classifier. The feature extraction is included in this notebook, even for the features we did not keep.\n",
    "\n",
    "**Performance summary of some of the other attempted experiments - measured on the random split:**\n",
    "**Performance with the random split:**\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th><b>Experiment</b></th>\n",
    "    <th><b>F1 (Kermit)</b></th>\n",
    "    <th><b>Precision (Kermit)</b></th>\n",
    "    <th><b>Recall (Kermit)</b></th>\n",
    "    <th><b>F1 (Waldorf & Statler)</b></th>\n",
    "    <th><b>Precision (Waldorf & Statler)</b></th>\n",
    "    <th><b>Recall (Waldorf & Statler)</b></th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td><b>RMS+ZCR+13 MFCCs</b></td> <!-- Experiment -->\n",
    "    <td>0.5387</td> <!-- F1 Kermit -->\n",
    "    <td>0.6737</td> <!-- Prec Kermit -->\n",
    "    <td>0.4487</td> <!-- Rec Kermit -->\n",
    "    <td>0.1417</td> <!-- F1 W&S -->\n",
    "    <td>0.5915</td> <!-- Prec W&S -->\n",
    "    <td>0.0805</td> <!-- Rec W&S -->\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>RMS+ZCR+7 Spectral contrasts</b></td> <!-- Experiment -->\n",
    "    <td>0.1816</td> <!-- F1 Kermit -->\n",
    "    <td>0.4711</td> <!-- Prec Kermit -->\n",
    "    <td>0.1124</td> <!-- Rec Kermit -->\n",
    "    <td>0</td> <!-- F1 W&S -->\n",
    "    <td>0</td> <!-- Prec W&S -->\n",
    "    <td>0</td> <!-- Rec W&S -->\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>RMS+ZCR+Spectral centroid+Spectral roll-off</b></td> <!-- Experiment -->\n",
    "    <td>0.1760</td> <!-- F1 Kermit -->\n",
    "    <td>0.4573</td> <!-- Prec Kermit -->\n",
    "    <td>0.1090</td> <!-- Rec Kermit -->\n",
    "    <td>0</td> <!-- F1 W&S -->\n",
    "    <td>0</td> <!-- Prec W&S -->\n",
    "    <td>0</td> <!-- Rec W&S -->\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>RMS+ZCR+Chroma features</b></td> <!-- Experiment -->\n",
    "    <td>0.1396</td> <!-- F1 Kermit -->\n",
    "    <td>0.4530</td> <!-- Prec Kermit -->\n",
    "    <td>0.0825</td> <!-- Rec Kermit -->\n",
    "    <td>0</td> <!-- F1 W&S -->\n",
    "    <td>0</td> <!-- Prec W&S -->\n",
    "    <td>0</td> <!-- Rec W&S -->\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>RMS+ZCR+Pitch</b></td> <!-- Experiment -->\n",
    "    <td>0.3253</td> <!-- F1 Kermit -->\n",
    "    <td>0.4421</td> <!-- Prec Kermit -->\n",
    "    <td>0.2574</td> <!-- Rec Kermit -->\n",
    "    <td>0</td> <!-- F1 W&S -->\n",
    "    <td>0</td> <!-- Prec W&S -->\n",
    "    <td>0</td> <!-- Rec W&S -->\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision-based detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid approach combining the features from both modalities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: explain how we combined the features + classifier we chose\n",
    "\n",
    "TODO: discuss results\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
