{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Kermit and Waldorf & Statler based on audio features\n",
    "\n",
    "This notebook documents our approach to detect Kermit and Waldorf and Statler based on audio feature-engineering.  \n",
    "We employ a Logistic Regression Classifier to predict the characters based on different audio-engineering features used for the different characters.\n",
    "\n",
    "## Time sheet for this notebook\n",
    "\n",
    "**Daniel Blasko:**\n",
    "| Date | Task | Hours |\n",
    "| --- | --- | --- |\n",
    "| 27.11.2023 | Setup notebook, first experiments | 3 |\n",
    "| 27.11.2023 | Implement \"utils/MuppetDataset.py\" that generally loads and handles the annotated video data | 1 |\n",
    "| 28.11.2023 | Experiment & build feature extraction for both characters, align audio samples with frame annotations | 2 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa.feature as lf\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from utils.MuppetDataset import MuppetDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "Set the booleans below to extract the audio/frames from the .avi files if it has not been done previously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_audio = True\n",
    "extract_frames = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_paths = [\n",
    "    \"../data/Muppets-02-01-01.avi\",\n",
    "    \"../data/Muppets-02-04-04.avi\",\n",
    "    \"../data/Muppets-03-04-03.avi\",\n",
    "]\n",
    "annotation_paths = [\n",
    "    \"../data/GroundTruth_Muppets-02-01-01.csv\",\n",
    "    \"../data/GroundTruth_Muppets-02-04-04.csv\",\n",
    "    \"../data/GroundTruth_Muppets-03-04-03.csv\",\n",
    "]\n",
    "\n",
    "dataset = MuppetDataset(video_paths, annotation_paths, extract_audio, extract_frames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example for handling the data for video 0:\n",
    "\n",
    "```python\n",
    "dataset.audio_paths[0]\n",
    "dataset.audios[0]\n",
    "dataset.annotations.loc[dataset.annotations.Video == 0]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio feature extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligning audio features with video frame rate\n",
    "\n",
    "The annotations are at the video frame level, for audio too. Therefore, we need to align the audio features with the video frames.\n",
    "\n",
    "We start by checking the framerate of the videos and remind of our audio sampling rate:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/1\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ffprobe -v error -select_streams v:0 -show_entries stream=avg_frame_rate -of default=noprint_wrappers=1:nokey=1 ../data/Muppets-02-01-01.avi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMES_PER_SECOND = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUDIO_SAMPLING_RATE = dataset.audios[0][\"sr\"]\n",
    "AUDIO_SAMPLING_RATE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 25 frames per second, and the 16k audio samples per second.  \n",
    "We therefore have $\\frac{44100}{25} = 1764$ audio samples per frame and divide our audio features in windows of 1764 samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_length_in_frames = dataset.annotations.loc[\n",
    "    dataset.annotations.Video == 0\n",
    "].Frame_number.max()\n",
    "\n",
    "\n",
    "video_duration_seconds = video_length_in_frames / FRAMES_PER_SECOND\n",
    "required_audio_length = int(video_duration_seconds * AUDIO_SAMPLING_RATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features for Kermit\n",
    "\n",
    "We make our decisions based on the observation that Kermit displays a distinct audio pattern where his interventions start with screaming, and transition to mumbling as he speaks, which should correspond to a high foundational frequency.  \n",
    "We therefore decide to extract the fundational frequency of the audio (pitch), as well as loudness.\n",
    "\n",
    "**We normalize all extracted features.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for audio in dataset.audios:\n",
    "    # Pad audio with silence to extract features from the last 8 frames\n",
    "    audio[\"audio\"] = np.pad(\n",
    "        audio[\"audio\"],\n",
    "        (0, required_audio_length - dataset.audios[0][\"audio\"].shape[0]),\n",
    "        \"constant\",\n",
    "    )\n",
    "    # Loudness (through RMS energy)\n",
    "    audio[\"loudness_rms\"] = librosa.util.normalize(\n",
    "        librosa.feature.rms(\n",
    "            y=audio[\"audio\"], hop_length=int(audio[\"sr\"] / FRAMES_PER_SECOND)\n",
    "        )[0]\n",
    "    )\n",
    "    # Zero crossing rate\n",
    "    audio[\"zcr\"] = librosa.util.normalize(\n",
    "        librosa.feature.zero_crossing_rate(\n",
    "            y=audio[\"audio\"], hop_length=int(audio[\"sr\"] / FRAMES_PER_SECOND)\n",
    "        )[0]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach with the `hop_length` of 1764 samples leads to feature vectors of the length of the number of frames, which is what we desired.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features for Waldorf & Statler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simmod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
